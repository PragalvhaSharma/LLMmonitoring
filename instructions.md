
1
How To Monitor My Agent üîç
You will build a platform for one of the important pillars of building AI agents: 
monitoring and observability. Your task is to build a simplified version of how a monitoring service tool would 
work by ‚Äòmocking º events using an LLM (in the real world, these would be 
conversations), and creating a simple API that will ingest those events and create 
reports.
Time estimate: 2ÓÇà3 hours. Quality is more important but bonus points for being 
quick.
Time limit: 24 hours from the moment you receive this guide. 
Overview
You will build and deploy two Dockerized TypeScript services on any cloud 
provider of your choice with bonus points for deploying on AWS. Please include 
an explanation of your chosen cloud architecture and how the services integrate 
regardless of the platform you select.ÓÅ≤ÓÇî Service A ÓÇÅIngest & Metrics)
Accepts simulated LLM-agent events, maintains rolling aggregates (last 5 
minutes or 30 events), and exposes Prometheus-style and JSON metrics.ÓÅ≥ÓÇî Service B ÓÇÅSimulator)
Every minute calls an LLM API to generate one plausible event and POST(s) it 
to Service A may optionally have robust error catching and latency 
optimizations.Both services must be publicly accessible so we can verify end-to-end 
functionality.
Service A: Ingest & Metrics
Endpoints
ÓÅ≤ÓÇî POST /ingestHow To Monitor My Agent üîç1
Accepts one event JSONÓÇí
{
"timestamp": "2025ÓÇà06ÓÇà10T12ÓÇí34ÓÇí56.789Z",
"sessionId": "abc123",
"intent": "BookFlight",
"latencyMs"ÓÇí 120,
"success": true,
"confidence"ÓÇí 0.87
}
Validate fields; return 400 on invalid payload.Store in a rolling window of the last 5 minutes or the most recent 30 events 
(whichever yields fewer).ÓÅ≥ÓÇî GET /metrics
Returns Prometheus-like (can be different) plaintext metrics (e.g. runtime, latency, 
cost metrics):
voice_events_total 42
voice_latency_ms_avg 135.4
voice_error_rate 0.095
voice_confidence_avg 0.88
ÓÅ≥ÓÇî GET /report
Returns JSON summary:
{
"totalEvents"ÓÇí 42,
"avgLatencyMs"ÓÇí 135.4,
"errorRate"ÓÇí 0.095,
"avgConfidence"ÓÇí 0.88
}How To Monitor My Agent üîç2
Rolling Aggregates
Maintain in memory (e.g. deque or ring buffer) constrained by:ÓÅ≤ÓÇî Time window: events within the last 5 minutesÓÅ≥ÓÇî Count window: up to the most recent 30 eventsUse whichever filter results in fewer events for your calculations.Metrics to compute:
totalEvents
avgLatencyMs
errorRate ÓÇõ 1 ÓÇû (successCount / totalEvents)
avgConfidence
Packaging & Deployment
ÓÅ≤ÓÇî Node.js ÓÇù TypeScriptÓÅ≥ÓÇî Docker image listens on port 8080ÓÅ¥ÓÇî Deploy it to the cloud, preferably to AWSÓÅµÓÇî Provide public URLs for /ingest, /metrics, /report
Service B: Event Simulator
Behavior
ÓÅ≤ÓÇî Every minute, call an LLM through an API to generate one JSON event 
matching Service A ºs schema.aÓÇî We recommend you use Mistral ºs or Grok ºs API, as they each provide free 
tokens, but you may choose any provider you know best, keeping in mind 
the optional optimization challenge. Example:
curl https://api.groq.com/openai/v1/chat/completions -s \ÓÇàH "Content-Type: application/json" \ÓÇàH "Authorization: Bearer <>" \
-d '{How To Monitor My Agent üîç3
"model": "meta-llama/llama-4-scout-17b-16e-instruct",
"messages": [{
    "role": "user",
    "content": "Explain the importance of fast language models"
}]
}'
ÓÅ≥ÓÇî POST the generated event to Service A ºs /ingest URL.Bonus: Optimize for latency/error handling Service ºs B response. 
Packaging & Deployment
ÓÅ≤ÓÇî Node.js ÓÇù TypeScriptÓÅ≥ÓÇî Config via environment variables:
LLM_OF_CHOICE_KEY
TARGET_URL ÓÇÅService A ºs /ingest URLÓÇÇ
ÓÅ¥ÓÇî Docker image that launches the simulation on container startÓÅµÓÇî Deploy to Cloud Run with a public endpoint
README Requirements
Clearly specify:ÓÅ≤ÓÇî How to call and test each endpoint (/ingest, /metrics, /report) using Cloud Run 
URLsÓÅ≥ÓÇî Anything else you would like us to know about your 
implementation/challenges and how you approached them. ÓÅ¥ÓÇî Any optimizations or thoughtful error handling you implemented.
Submission
ÓÅ≤ÓÇî Create a private GitHub repository and grant collaborator access to:
kaitakami  SomeoneElseSt  donadolHow To Monitor My Agent üîç4
ÓÅ≥ÓÇî Include all source code, Dockerfiles, and README.ÓÅ¥ÓÇî Provide the public Cloud Run URLs for both services.ÓÅµÓÇî Indicate approximate time spent (this helps us understand how long 
candidates typically take, though it won't influence our decision)ÓÅ∂ÓÇî Loom video no longer than five minutes with you covering:aÓÇî A flowchart diagram of how data flows through your system and high-level 
explanation of how it works. If you did not use AWS you must also explain 
how the system you used integrates into the services. Example: